{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0OSlSAXjEoX"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "THE DIRECTORY NAMES USED TO STORE THE DATA EXPLAIN THE STRATEGY TO CREATE THE GRAPH:\n",
        "\n",
        "PSS is the number of PAST SAME SPEAKER considered for each node (e.g. 3PSS means 3 pasts same speaker)\n",
        "PDS is the number of PAST DIFFERENT SPEAKER considered for each node (e.g. 8PDS means 8 pasts different speaker)\n",
        "Similarly there are FSS and FDS\n",
        "\n",
        "\"timestamp\" or \"reply_to\" explain how the concept of past and future comments for a specific node is defined:\n",
        "- timestamp: past and future comments (nodes) are identified by timestamp independently on the tree structure of the\n",
        "  conversation, this means that a non-main-branch comment of the conversation can be linked with comments of the main branch\n",
        "- reply_to: past and future comments (nodes) are identified by the reply_to field in the dataset, which considers\n",
        "  the structure of the conversation\n",
        "- similarity: nodes are linked dending on how similar the text is\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OK6-gwC_r1V4"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "TO DO:\n",
        "- check the results on a balanced dataset\n",
        "- check the results when going in the past follows back the tree structure of the conversation (reply_to)\n",
        "- check for models that apply explainability on GNNs\n",
        "- is there a different way to represent/preprocess text\n",
        "- check for results with different features, POS tags, emotions, number of words, number of deltas, distance from last OP utterance etc...\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ui-Wcv1NSClZ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5j_n1y3pKyn"
      },
      "outputs": [],
      "source": [
        "!pip install torch-geometric\n",
        "!pip3 install convokit\n",
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEHlnUcvVR3U"
      },
      "outputs": [],
      "source": [
        "#!pip install torch-spline-conv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQKhDI1XDZu2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "from torch_geometric.nn import GCNConv, GATConv, SAGEConv, CGConv, SplineConv\n",
        "from torch_geometric.nn import BatchNorm, global_add_pool\n",
        "from torch.nn import Embedding, Linear, ModuleList, ReLU, Sequential\n",
        "from torch_geometric.nn import GCN, GAT, DeepGCNLayer, GENConv\n",
        "from torch_geometric.utils import degree\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, roc_auc_score, average_precision_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.tokenize import word_tokenize\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from convokit import Corpus, download\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import emoji\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import traceback\n",
        "import sys\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import statistics\n",
        "import copy\n",
        "import string\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import pipeline\n",
        "from scipy.special import softmax\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "IiDueb9wqHnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTbXYivckWVk"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NwvSy-9bHN5"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVLB_zMZuoqR"
      },
      "outputs": [],
      "source": [
        "# Load Universal Sentence Encoder model\n",
        "universalSentenceEncoder = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sonXaluxm6ap"
      },
      "outputs": [],
      "source": [
        "robertaModel = f\"cardiffnlp/twitter-roberta-base-emotion\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(robertaModel)\n",
        "tfmodel = TFAutoModelForSequenceClassification.from_pretrained(robertaModel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zof5Rwt7QCrh"
      },
      "outputs": [],
      "source": [
        "# see https://github.com/CornellNLP/ConvoKit/blob/master/datasets/winning-args-corpus/stats.ipynb for pre-processing\n",
        "corpus = Corpus(filename=download(\"winning-args-corpus\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqA8ENHqS2cz"
      },
      "outputs": [],
      "source": [
        "#cIds = corpus.get_conversation_ids()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqnBq6LMg3pO"
      },
      "outputs": [],
      "source": [
        "## START: HELPERS\n",
        "\n",
        "def featureMustBeRecomputed(cDf, featureName):\n",
        "  return featureName not in cDf.columns or alwaysCompute == True\n",
        "\n",
        "# outputs scores of emotions in this order [anger, joy, optimism, sadness]\n",
        "# (each score goes from 0 low intensity, to 1 high intensity)\n",
        "def getEmotionsScores(text):\n",
        "    em_labels = ['anger', 'joy', 'optimism', 'sadness']\n",
        "\n",
        "    encoded_input = tokenizer(text, return_tensors='tf')\n",
        "    output = tfmodel(encoded_input)\n",
        "    scores = output[0][0].numpy()\n",
        "    scores = softmax(scores).tolist()\n",
        "\n",
        "    return scores\n",
        "\n",
        "def getSpeakersDeltaFromComment(utteranceIndex):\n",
        "    numOfDeltasStringUnclean = corpus.get_utterance(utteranceIndex).meta['author_flair_text']\n",
        "    numOfDeltas = 0\n",
        "    if numOfDeltasStringUnclean != None:\n",
        "      try:\n",
        "        # cleans the string from non-digits characters and then converts to int\n",
        "        numOfDeltas = int(list(filter(str.isdigit, numOfDeltasStringUnclean))[0])\n",
        "      except:\n",
        "        return 0\n",
        "\n",
        "    return numOfDeltas\n",
        "\n",
        "def getWordsOfSentenceAsList(sentence):\n",
        "  return list(str(sentence).lower().split())\n",
        "\n",
        "def getSentenceNumOfWords(sentence):\n",
        "    if sentence == None:\n",
        "      return 0\n",
        "\n",
        "    return len(getWordsOfSentenceAsList(sentence))\n",
        "\n",
        "def numberOfcommonWordsBetweenTwoListOfWords(s1, s2):\n",
        "  commonWords = list(set(s1).intersection(s2))\n",
        "  return len(commonWords)\n",
        "\n",
        "def getStopwordsInWordsList(s1):\n",
        "  return [token for token in s1 if token in stop_words]\n",
        "\n",
        "def getContentWordsInWordsList(s1):\n",
        "  return [token for token in s1 if token not in stop_words]\n",
        "\n",
        "def getRightTimeDiff(tdiff,medianTimeDiff):\n",
        "    if tdiff == 0:\n",
        "      return medianTimeDiff\n",
        "\n",
        "    return tdiff\n",
        "\n",
        "def jaccard_similarity(doc1, doc2):\n",
        "    if doc1 == '' or doc2 == '':\n",
        "      return 0.0\n",
        "\n",
        "    if type(doc1) == list:\n",
        "      words_doc_1 = set(doc1)\n",
        "      words_doc_2 = set(doc2)\n",
        "    else:\n",
        "      # List the unique words in a document\n",
        "      words_doc_1 = set(doc1.lower().split())\n",
        "      words_doc_2 = set(doc2.lower().split())\n",
        "\n",
        "    # Find the intersection of words between documents\n",
        "    intersection = words_doc_1.intersection(words_doc_2)\n",
        "\n",
        "    # Find the union of words between documents\n",
        "    union = words_doc_1.union(words_doc_2)\n",
        "\n",
        "    # Jaccard Similarity\n",
        "    try:\n",
        "      jaccardSimilarityCoefficient = 0 if len(union) == 0 else float(len(intersection)) / len(union)\n",
        "    except:\n",
        "      print('error while calculating Jaccard similarity')\n",
        "      print('intersection: ' + str(len(intersection)))\n",
        "      print('union: ' + str(len(union)))\n",
        "      jaccardSimilarityCoefficient = 0\n",
        "    return jaccardSimilarityCoefficient\n",
        "\n",
        "## END: HELPERS OF EDGE ATTRIBUTE FUNCTIONS\n",
        "\n",
        "#------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "## START: EDGE ATTRIBUTE FUNCTIONS\n",
        "\n",
        "# computes the similarity of two sentences, by default uses the jaccard similarity\n",
        "def text_sim(cDf, edgesAsIndicesList, similarityFunction = jaccard_similarity):\n",
        "  similarityList = []\n",
        "\n",
        "  for edge in edgesAsIndicesList:\n",
        "    n1_index = edge[0]\n",
        "    n2_index = edge[1]\n",
        "\n",
        "    node1Text = cDf.loc[n1_index]['text']\n",
        "    node2Text = cDf.loc[n2_index]['text']\n",
        "\n",
        "    similarityList.append(similarityFunction(node1Text, node2Text))\n",
        "\n",
        "  return similarityList\n",
        "\n",
        "\n",
        "# computes the ratio of the deltas of the speakers of the two linked comments\n",
        "def delta_ratio(cDf, edgesAsIndicesList):\n",
        "  deltaRatioList = []\n",
        "\n",
        "  for edge in edgesAsIndicesList:\n",
        "    n1_index = edge[0]\n",
        "    n2_index = edge[1]\n",
        "\n",
        "    speaker1Deltas = getSpeakersDeltaFromComment(n1_index)\n",
        "    speaker2Deltas = getSpeakersDeltaFromComment(n2_index)\n",
        "\n",
        "    if speaker2Deltas == 0:\n",
        "      deltaRatioList.append(speaker1Deltas)\n",
        "      continue\n",
        "\n",
        "    deltaRatioList.append(speaker1Deltas / speaker2Deltas)\n",
        "\n",
        "  return deltaRatioList\n",
        "\n",
        "\n",
        "# computes the ratio between the number of words of one node compared to the linked one\n",
        "def word_len_ratio(cDf, edgesAsIndicesList):\n",
        "  deltaRatioList = []\n",
        "\n",
        "  for edge in edgesAsIndicesList:\n",
        "    n1_index = edge[0]\n",
        "    n2_index = edge[1]\n",
        "\n",
        "    node1Text = cDf.loc[n1_index].text\n",
        "    node2Text = cDf.loc[n2_index].text\n",
        "\n",
        "    numOfWords1 = getSentenceNumOfWords(node1Text)\n",
        "    numOfWords2 = getSentenceNumOfWords(node2Text)\n",
        "\n",
        "    if numOfWords2 == 0:\n",
        "      deltaRatioList.append(numOfWords1)\n",
        "      continue\n",
        "\n",
        "    deltaRatioList.append(numOfWords1 / numOfWords2)\n",
        "\n",
        "  return deltaRatioList\n",
        "\n",
        "\n",
        "# computes the difference between the timestamps of two different nodes\n",
        "def time_diff(cDf, edgesAsIndicesList):\n",
        "  timeDiffList = []\n",
        "  timeDiffListDebug = []\n",
        "  for edge in edgesAsIndicesList:\n",
        "    n1_index = edge[0]\n",
        "    n2_index = edge[1]\n",
        "    try:\n",
        "      node1Timestamp = int(cDf.loc[n1_index]['timestamp'])\n",
        "      node2Timestamp = int(cDf.loc[n2_index]['timestamp'])\n",
        "      if node1Timestamp == 0 or node2Timestamp == 0:\n",
        "        timeDiff = 0\n",
        "      else:\n",
        "        timeDiff = abs(node1Timestamp - node2Timestamp)\n",
        "    except:\n",
        "      # this can happen when timestamp is NaN and we try to convert it to int\n",
        "      timeDiff = 0\n",
        "\n",
        "    timeDiffList.append(timeDiff)\n",
        "    timeDiffListDebug.append([n1_index, n2_index, timeDiff])\n",
        "\n",
        "  timeDiffListOriginal = copy.deepcopy(timeDiffList)\n",
        "  # for all values linked to the first node (which doesn't have a timestamp) set the distance as the median value of the sorted list\n",
        "\n",
        "  timeDiffList.sort()\n",
        "  medianTimeDiff = statistics.median(timeDiffList)\n",
        "\n",
        "  timeDiffListCleaned = [getRightTimeDiff(timeDiffEl, medianTimeDiff) for timeDiffEl in timeDiffListOriginal]\n",
        "  return timeDiffListCleaned\n",
        "\n",
        "# add a relation type 0 (same_speaker), 1 (time_related) NOT COMPATIBLE WITH SplineNN\n",
        "def relation_type(cDf, edgesAsIndicesList):\n",
        "  relationTypesList = []\n",
        "  relationTypesListDebug = []\n",
        "  for edge in edgesAsIndicesList:\n",
        "    n1_index = edge[0]\n",
        "    n2_index = edge[1]\n",
        "\n",
        "    node1Speaker = cDf.loc[n1_index]['speaker']\n",
        "    node2Speaker = cDf.loc[n2_index]['speaker']\n",
        "    if node1Speaker == node2Speaker:\n",
        "      relationTypesList.append(0)\n",
        "      relationTypesListDebug.append([n1_index, n2_index, 0])\n",
        "    else:\n",
        "      relationTypesList.append(1)\n",
        "      relationTypesListDebug.append([n1_index, n2_index, 1])\n",
        "\n",
        "  relationTypesNpArr = np.array(relationTypesList)\n",
        "  return torch.nn.functional.one_hot(torch.tensor(relationTypesNpArr), num_classes=2).float()\n",
        "\n",
        "## END: EDGE ATTRIBUTE FUNCTIONS\n",
        "\n",
        "#------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "## START: EDGE NODE FEATURES FUNCTIONS\n",
        "\n",
        "# computes a node feature that has no information\n",
        "# used to compare a model having only with edge_attr and no node_features\n",
        "def empty(cDf):\n",
        "  emptyList = []\n",
        "  for index, utterance in cDf.iterrows():\n",
        "    emptyList.append(0)\n",
        "\n",
        "  return emptyList\n",
        "\n",
        "# computes the sentence embedding of each element in the dataframe\n",
        "def embeddings(cDf):\n",
        "  featureName = 'embeddings'\n",
        "  # if the embedding was already calculated\n",
        "  if not featureMustBeRecomputed(cDf, featureName):\n",
        "    embeddingList = cDf[featureName].tolist()\n",
        "  else:\n",
        "    embeddingList = []\n",
        "    cDf[featureName] = ''\n",
        "    for index, utterance in cDf.iterrows():\n",
        "      # 1. Extract node features from each utterance\n",
        "      try:\n",
        "        tokens = word_tokenize(utterance.text)\n",
        "      except Exception as e:\n",
        "        print('EXCEPTION: ')\n",
        "        print(e)\n",
        "        print(traceback.format_exc())\n",
        "        print('')\n",
        "        print('UTTERANCE')\n",
        "        print(utterance)\n",
        "        print(utterance.text)\n",
        "      # Convert tokens to sentence\n",
        "      sentence = ' '.join(tokens)\n",
        "      # Use Universal Sentence Encoder to obtain utterance embedding of size (512 => an array of 512 numbers)\n",
        "      utteranceEmbedding = list(universalSentenceEncoder([sentence])[0].numpy())\n",
        "      cDf.at[index, featureName] = utteranceEmbedding\n",
        "      embeddingList.append(utteranceEmbedding)\n",
        "\n",
        "  return embeddingList\n",
        "\n",
        "\n",
        "def sentiment(cDf):\n",
        "  sid = SentimentIntensityAnalyzer()\n",
        "  sentimentList = []\n",
        "\n",
        "  for index, utterance in cDf.iterrows():\n",
        "    sentiment_scores = sid.polarity_scores(utterance.text)\n",
        "    # Positive sentiment score\n",
        "    pos_score = sentiment_scores['pos']\n",
        "    # Negative sentiment score\n",
        "    neg_score = sentiment_scores['neg']\n",
        "    # Neutral sentiment score\n",
        "    neu_score = sentiment_scores['neu']\n",
        "    sentimentList.append([pos_score, neg_score, neu_score])\n",
        "\n",
        "  return sentimentList\n",
        "\n",
        "\n",
        "# gets the number of delta of a speaker\n",
        "def speaker_delta(cDf):\n",
        "  speakerDeltaList = []\n",
        "  for index, utterance in cDf.iterrows():\n",
        "    numOfDeltas = getSpeakersDeltaFromComment(index)\n",
        "    speakerDeltaList.append(numOfDeltas)\n",
        "\n",
        "  return speakerDeltaList\n",
        "\n",
        "\n",
        "# gets absolute position in a conversation\n",
        "def abs_position(cDf):\n",
        "  positionList = []\n",
        "  i = 0\n",
        "  for index, utterance in cDf.iterrows():\n",
        "    positionList.append(i)\n",
        "    i += 1\n",
        "\n",
        "  return positionList\n",
        "\n",
        "\n",
        "# gets the number of words of a comment\n",
        "def word_len(cDf):\n",
        "  wordLenList = []\n",
        "  i = 0\n",
        "  for index, utterance in cDf.iterrows():\n",
        "    numOfWords = getSentenceNumOfWords(utterance.text)\n",
        "    wordLenList.append(numOfWords)\n",
        "\n",
        "  return wordLenList\n",
        "\n",
        "\n",
        "# gets the number of words of a comment\n",
        "def emotions(cDf):\n",
        "  emotionsList = []\n",
        "  featureName = 'emotions'\n",
        "\n",
        "  if featureMustBeRecomputed(cDf, featureName):\n",
        "    cDf[featureName] = ''\n",
        "\n",
        "  for index, utterance in cDf.iterrows():\n",
        "    if utterance[featureName] == '':\n",
        "      try:\n",
        "        if len(utterance.text) > 1000:\n",
        "          midTextIndex = int(len(utterance.text) / 2)\n",
        "          t1 = np.array(getEmotionsScores(utterance.text[0:midTextIndex]))\n",
        "          t2 = np.array(getEmotionsScores(utterance.text[midTextIndex:-1]))\n",
        "          fourEmotions = list(t1 + t2 / 2)\n",
        "        else:\n",
        "          fourEmotions = getEmotionsScores(utterance.text)\n",
        "      except:\n",
        "        fourEmotions = [0,0,0,0]\n",
        "        print('Exception in emotions()')\n",
        "        print([fourEmotions, utterance.text])\n",
        "\n",
        "      cDf.at[index, featureName] = fourEmotions\n",
        "    else:\n",
        "      fourEmotions = utterance[featureName]\n",
        "    emotionsList.append(fourEmotions)\n",
        "\n",
        "  return emotionsList\n",
        "\n",
        "\n",
        "# gets the number of words of a comment\n",
        "def tf_idf(cDf, maxScores = 5):\n",
        "  tfIdfList = []\n",
        "  utteranceList = cDf['text'].map(preprocess_comment).map(lambda t: t.lower()).tolist()\n",
        "\n",
        "  vectorizer = TfidfVectorizer()\n",
        "  X = vectorizer.fit_transform(utteranceList)\n",
        "  vectorizer.get_feature_names_out()\n",
        "\n",
        "  tfIdfDf = pd.DataFrame(X[0].T.todense(), index=vectorizer.get_feature_names_out(), columns=[\"tf-idf\"])\n",
        "\n",
        "  avgTfIdfScore = float(tfIdfDf['tf-idf'].mean())\n",
        "\n",
        "  for index, utterance in cDf.iterrows():\n",
        "    utteranceScores = []\n",
        "    utteranceSentenceCleaned = preprocess_comment(utterance.text)\n",
        "    utteranceWords = utteranceSentenceCleaned.lower().split()\n",
        "    for word in utteranceWords:\n",
        "      try:\n",
        "        wordScoreSeries = tfIdfDf.loc[word]\n",
        "\n",
        "        if wordScoreSeries.shape[0] == 0:\n",
        "          wordScore = avgTfIdfScore\n",
        "        else:\n",
        "          wordScore = float(wordScoreSeries['tf-idf'])\n",
        "      except:\n",
        "         wordScore = avgTfIdfScore\n",
        "      # if the word does not exist in the td-idf dictionary add it as average score\n",
        "\n",
        "      utteranceScores.append(wordScore)\n",
        "\n",
        "    utteranceScores.sort()\n",
        "    utteranceScores.sort(reverse=True)\n",
        "    utterancesBestScores = utteranceScores[0:maxScores]\n",
        "\n",
        "    # if the sentence is shorter than 10 words append some empty scores\n",
        "    for i in range(10 - len(utterancesBestScores)):\n",
        "      utterancesBestScores.append(avgTfIdfScore)\n",
        "\n",
        "    # sort again after appending scores if an utterance had less than 10 words\n",
        "    utterancesBestScores.sort(reverse=True)\n",
        "\n",
        "    tfIdfList.append(utterancesBestScores)\n",
        "\n",
        "  return tfIdfList\n",
        "\n",
        "\n",
        "def interplay(cDf):\n",
        "  originalPostUtt = cDf.iloc[0]\n",
        "  wordsInOpAll = getWordsOfSentenceAsList(originalPostUtt.text)\n",
        "  stopwordsOfOp = getStopwordsInWordsList(wordsInOpAll)\n",
        "\n",
        "  opWithNoPunctuation = originalPostUtt.text.translate(str.maketrans('', '', string.punctuation))\n",
        "  wordsInOpWithNoPunctuation = getWordsOfSentenceAsList(opWithNoPunctuation)\n",
        "  contentWordsOfOp = getContentWordsInWordsList(wordsInOpWithNoPunctuation)\n",
        "\n",
        "  interplayList = []\n",
        "  i = 0\n",
        "  for index, utterance in cDf.iterrows():\n",
        "    wordsInArgumentAll = getWordsOfSentenceAsList(utterance.text)\n",
        "    argumentWithNoPunctuation = utterance.text.translate(str.maketrans('', '', string.punctuation))\n",
        "    wordsInArgumentWithNoPunctuation = getWordsOfSentenceAsList(argumentWithNoPunctuation)\n",
        "    contentWordsOfOp = getContentWordsInWordsList(wordsInOpWithNoPunctuation)\n",
        "\n",
        "    numOfcommonInAll = numberOfcommonWordsBetweenTwoListOfWords(wordsInOpAll, wordsInArgumentAll)\n",
        "    replyFracInAll = 0 if len(wordsInArgumentAll) == 0 else numOfcommonInAll / len(wordsInArgumentAll)\n",
        "\n",
        "    opFracInAll = 0 if len(wordsInOpAll) == 0 else numOfcommonInAll / len(wordsInOpAll)\n",
        "\n",
        "    contentWordsOfArgument = getContentWordsInWordsList(wordsInArgumentWithNoPunctuation)\n",
        "    numOfCommonWordsInContent = numberOfcommonWordsBetweenTwoListOfWords(contentWordsOfOp, contentWordsOfArgument)\n",
        "    replyFracInContent = 0 if len(contentWordsOfArgument) == 0 else numOfCommonWordsInContent / len(contentWordsOfArgument)\n",
        "\n",
        "    stopwordsOfArgument = getStopwordsInWordsList(wordsInArgumentAll)\n",
        "    numOfCommonStopwords = numberOfcommonWordsBetweenTwoListOfWords(stopwordsOfOp, stopwordsOfArgument)\n",
        "    replyFracInStopwords = 0 if len(stopwordsOfArgument) == 0 else numOfCommonStopwords / len(stopwordsOfArgument)\n",
        "\n",
        "    opFracInStopwords = 0 if len(stopwordsOfOp) == 0 else numOfCommonStopwords / len(stopwordsOfOp)\n",
        "\n",
        "    jaccardInContent = jaccard_similarity(contentWordsOfArgument, contentWordsOfOp)\n",
        "    jaccardInStopwords = jaccard_similarity(stopwordsOfArgument, stopwordsOfOp)\n",
        "\n",
        "    interplayList.append([\n",
        "        numOfcommonInAll,\n",
        "        replyFracInAll,\n",
        "        opFracInAll,\n",
        "        numOfCommonWordsInContent,\n",
        "        replyFracInContent,\n",
        "        numOfCommonStopwords,\n",
        "        replyFracInStopwords,\n",
        "        opFracInStopwords,\n",
        "        jaccardInContent,\n",
        "        jaccardInStopwords\n",
        "        ])\n",
        "\n",
        "  return interplayList\n",
        "\n",
        "dimensionalityMap = {\n",
        "    \"embeddings\": 512,\n",
        "    \"tf_idf\": 10,\n",
        "    \"interplay\": 10,\n",
        "    \"emotions\": 4,\n",
        "    \"sentiment\": 3,\n",
        "    \"empty\": 1,\n",
        "    \"abs_position\": 1,\n",
        "    \"word_len\": 1,\n",
        "    \"speaker_delta\": 1\n",
        "}\n",
        "\n",
        "\n",
        "## END: EDGE NODE FEATURES FUNCTIONS\n",
        "\n",
        "#------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "## START: CONF VARIABLES\n",
        "pss = 5\n",
        "pds = 0\n",
        "fss = 0\n",
        "fds = 0\n",
        "\n",
        "alwaysCompute = False\n",
        "\n",
        "#fetchMode = ['reply_to']\n",
        "#fetchMode = ['reply_to','timestamp']\n",
        "fetchMode = ['timestamp']\n",
        "#fetchMode = ['timestamp']\n",
        "\n",
        "#edgeAttributeFunctions = [relation_type]\n",
        "#edgeAttributeFunctions = [time_diff, relation_type]\n",
        "#edgeAttributeFunctions = [time_diff, text_sim, delta_ratio, word_len_ratio]\n",
        "#edgeAttributeFunctions = [time_diff]\n",
        "edgeAttributeFunctions = []\n",
        "\n",
        "#nodeFeaturesFunctions = [embeddings]\n",
        "#nodeFeaturesFunctions = [tf_idf]\n",
        "#nodeFeaturesFunctions = [emotions]\n",
        "#nodeFeaturesFunctions = [speaker_delta]\n",
        "#nodeFeaturesFunctions = [abs_position]\n",
        "#nodeFeaturesFunctions = [word_len]\n",
        "#nodeFeaturesFunctions = [embeddings, tf_idf, word_len]\n",
        "#nodeFeaturesFunctions = [embeddings, tf_idf, emotions]\n",
        "#nodeFeaturesFunctions = [embeddings, tf_idf, emotions, speaker_delta, abs_position, word_len]\n",
        "#nodeFeaturesFunctions = [tf_idf, emotions]\n",
        "#nodeFeaturesFunctions = [embeddings, tf_idf, sentiment]\n",
        "nodeFeaturesFunctions = [embeddings, tf_idf]\n",
        "#nodeFeaturesFunctions = [embeddings, word_len]\n",
        "#nodeFeaturesFunctions = [interplay]\n",
        "#nodeFeaturesFunctions = [embeddings, emotions]\n",
        "#nodeFeaturesFunctions = [embeddings, speaker_delta, abs_position, word_len]\n",
        "#nodeFeaturesFunctions = [abs_position, word_len]\n",
        "#nodeFeaturesFunctions = [empty]\n",
        "\n",
        "speakerLevelLabel = False\n",
        "\n",
        "## END: CONF VARIABLES\n",
        "\n",
        "#------------------------------------------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "op = 'My name is mario compresoni and I am manager of Gasinplot'\n",
        "s2 = 'I do not Agree with you Mr compresoni, managers are not so good at Gasinplot'\n",
        "\n",
        "wordsInOpAll = getWordsOfSentenceAsList(op)\n",
        "opWithNoPunctuation = op.translate(str.maketrans('', '', string.punctuation))\n",
        "wordsInOpWithNoPunctuation = getWordsOfSentenceAsList(opWithNoPunctuation)\n",
        "contentWordsOfOp = getContentWordsInWordsList(wordsInOpWithNoPunctuation)\n",
        "stopwordsOfOp = getStopwordsInWordsList(wordsInOpAll)\n",
        "\n",
        "wordsInArgumentAll = getWordsOfSentenceAsList(s2)\n",
        "numOfcommonInAll = numberOfcommonWordsBetweenTwoListOfWords(wordsInOpAll, wordsInArgumentAll)\n",
        "replyFracInAll = numOfcommonInAll / len(wordsInArgumentAll)\n",
        "\n",
        "opFracInAll = numOfcommonInAll / len(wordsInOpAll)\n",
        "\n",
        "contentWordsOfArgument = getContentWordsInWordsList(wordsInArgumentAll)\n",
        "numOfCommonWordsInContent = numberOfcommonWordsBetweenTwoListOfWords(contentWordsOfOp, contentWordsOfArgument)\n",
        "replyFracInContent = numOfCommonWordsInContent / len(contentWordsOfArgument)\n",
        "\n",
        "stopwordsOfArgument = getStopwordsInWordsList(wordsInArgumentAll)\n",
        "numOfCommonStopwords = numberOfcommonWordsBetweenTwoListOfWords(stopwordsOfOp, stopwordsOfArgument)\n",
        "replyFracInStopwords = numOfCommonStopwords / len(stopwordsOfArgument)\n",
        "\n",
        "opFracInStopwords = numOfCommonStopwords / len(stopwordsOfOp)\n",
        "\n",
        "jaccardInContent = jaccard_similarity(contentWordsOfArgument, contentWordsOfOp)\n",
        "jaccardInStopwords = jaccard_similarity(stopwordsOfArgument, stopwordsOfOp)\n",
        "\n",
        "print([\n",
        "    numOfcommonInAll,\n",
        "    replyFracInAll,\n",
        "    opFracInAll,\n",
        "    numOfCommonWordsInContent,\n",
        "    replyFracInContent,\n",
        "    numOfCommonStopwords,\n",
        "    replyFracInStopwords,\n",
        "    opFracInStopwords,\n",
        "    jaccardInContent,\n",
        "    jaccardInStopwords\n",
        "    ])\n",
        "'''"
      ],
      "metadata": {
        "id": "DEhMZHlk4cHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnsMEbSyiAHe"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "$e = are the edge attributes names\n",
        "$l = is the way the links are constructed in the graph\n",
        "$n = are the node features names\n",
        "'''\n",
        "if len(edgeAttributeFunctions) > 0:\n",
        "  edgeAttributesStringJoined = '&'.join([f.__name__ for f in edgeAttributeFunctions])\n",
        "  edgeAttributesString = '$e=' + edgeAttributesStringJoined\n",
        "else:\n",
        "  edgeAttributesString = ''\n",
        "\n",
        "fetchModeStringJoined = '&'.join(fetchMode)\n",
        "fetchModeString = '$l=' + fetchModeStringJoined\n",
        "\n",
        "nodeFeaturesStringJoined = '-'.join([f.__name__ for f in nodeFeaturesFunctions])\n",
        "nodeFeaturesString = '$n=' + nodeFeaturesStringJoined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhjDaMnBq2MP"
      },
      "outputs": [],
      "source": [
        "baseDir = '/content/drive/My Drive/datasets/winning-args-corpus/'\n",
        "postfix = nodeFeaturesString + fetchModeString + '' + edgeAttributesString\n",
        "dirName = '$pss' + str(pss) + '$pds' + str(pds) + '$fss' + str(fss) + '$fds' + str(fds)  + postfix\n",
        "fullDirectory = baseDir + dirName\n",
        "try:\n",
        "  os.mkdir(fullDirectory)\n",
        "except Exception as e:\n",
        "  print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j604nKCQkIYb"
      },
      "outputs": [],
      "source": [
        "def preprocess_comment(comment, stop_words = stop_words):\n",
        "    # Replace links with \"URL\"\n",
        "    try:\n",
        "      comment = re.sub(r'http\\S+|www\\S+', 'URL', comment)\n",
        "      # Replace emoticons\n",
        "      comment = emoji.demojize(comment)\n",
        "      # Replace special chars\n",
        "      comment = re.sub(\"[^\\w\\s]\", \"\", comment)\n",
        "    except Exception as e:\n",
        "      print('EXC: ---')\n",
        "      print(e)\n",
        "      print(str(comment))\n",
        "      print(traceback.format_exc())\n",
        "      print('')\n",
        "\n",
        "    # Tokenize comment text and convert tokens to lowercase\n",
        "    tokens = [token.lower() for token in word_tokenize(comment)]\n",
        "    # Remove stopwords\n",
        "    # tokens = [token for token in tokens if token not in stop_words]\n",
        "    # Join tokens back into a comment text\n",
        "    preprocessed_comment = ' '.join(tokens)\n",
        "    return preprocessed_comment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "satiwnEXv8hX"
      },
      "outputs": [],
      "source": [
        "# Define the GCN model\n",
        "'''\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr = None):\n",
        "        if edge_attr != None:\n",
        "          x = self.conv1(x, edge_index, edge_attr)\n",
        "          x = F.leaky_relu(x)\n",
        "          x = self.conv2(x, edge_index, edge_attr)\n",
        "        else:\n",
        "          x = self.conv1(x, edge_index)\n",
        "          x = F.leaky_relu(x)\n",
        "          x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "\n",
        "class GAT(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, heads = 4):\n",
        "        super(GAT, self).__init__()\n",
        "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads)\n",
        "        self.conv2 = GATConv(hidden_channels * heads, out_channels, heads=1)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr = None):\n",
        "        if edge_attr != None:\n",
        "          x = self.conv1(x, edge_index, edge_attr)\n",
        "          x = F.leaky_relu(x)\n",
        "          x = self.conv2(x, edge_index, edge_attr)\n",
        "        else:\n",
        "          x = self.conv1(x, edge_index)\n",
        "          x = F.leaky_relu(x)\n",
        "          x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "'''\n",
        "\n",
        "class SplineNN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, dim = 1):\n",
        "        super().__init__()\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.convs.append(SplineConv(in_channels, hidden_channels, dim=dim, kernel_size=2))\n",
        "        self.convs.append(SplineConv(hidden_channels, out_channels, dim=dim, kernel_size=2))\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr = None, dropout = True):\n",
        "        conv1 = self.convs[0]\n",
        "        conv2 = self.convs[1]\n",
        "        x = F.dropout(x, training=self.training)\n",
        "\n",
        "        x = F.elu(conv1(x, edge_index, edge_attr))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = conv2(x, edge_index, edge_attr)\n",
        "        #return F.log_softmax(x, dim=1)\n",
        "        return x\n",
        "\n",
        "class DeeperGCN(torch.nn.Module):\n",
        "    def __init__(self, data, in_channels, hidden_channels, out_channels, num_layers):\n",
        "        super().__init__()\n",
        "\n",
        "        self.node_encoder = Linear(in_channels, hidden_channels)\n",
        "        self.edge_encoder = Linear(data.edge_attr.size(-1), hidden_channels)\n",
        "\n",
        "        self.layers = torch.nn.ModuleList()\n",
        "        for i in range(1, num_layers + 1):\n",
        "            conv = GENConv(hidden_channels, hidden_channels, aggr='softmax',\n",
        "                           t=1.0, learn_t=True, num_layers=2, norm='layer')\n",
        "            norm = nn.LayerNorm(hidden_channels, elementwise_affine=True)\n",
        "            act = ReLU(inplace=True)\n",
        "\n",
        "            layer = DeepGCNLayer(conv, norm, act, block='res+', dropout=0.1,\n",
        "                                 ckpt_grad=i % 3)\n",
        "            self.layers.append(layer)\n",
        "\n",
        "        self.lin = Linear(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr):\n",
        "        x = self.node_encoder(x)\n",
        "        edge_attr = self.edge_encoder(edge_attr)\n",
        "\n",
        "        x = self.layers[0].conv(x, edge_index, edge_attr)\n",
        "\n",
        "        for layer in self.layers[1:]:\n",
        "            x = layer(x, edge_index, edge_attr)\n",
        "\n",
        "        x = self.layers[0].act(self.layers[0].norm(x))\n",
        "        x = F.dropout(x, p=0.1, training=self.training)\n",
        "\n",
        "        return self.lin(x)\n",
        "\n",
        "\n",
        "class GSAGEv2(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, aggr = 'mean'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.convs.append(SAGEConv(in_channels, hidden_channels, aggr))\n",
        "        for _ in range(num_layers - 1):\n",
        "            self.convs.append(SAGEConv(hidden_channels, hidden_channels, aggr))\n",
        "        self.lin = nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr = None):\n",
        "        for i, conv in enumerate(self.convs):\n",
        "            x = conv(x, edge_index)\n",
        "            if i != self.num_layers - 1:\n",
        "                x = F.leaky_relu(x)\n",
        "                x = F.dropout(x, p=0.5, training=self.training)\n",
        "        return self.lin(x)\n",
        "\n",
        "\n",
        "class GSAGE(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(GSAGE, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels, 'mean')\n",
        "        self.conv2 = SAGEConv(hidden_channels, out_channels, 'mean')\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr = None):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.leaky_relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "class Baseline(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(Baseline, self).__init__()\n",
        "        self.fc1 = nn.Linear(in_channels, hidden_channels)\n",
        "        self.fc2 = nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index = None, edge_attr = None):\n",
        "        x = self.fc1(x)\n",
        "        x = F.leaky_relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUiQB61QGDC6"
      },
      "outputs": [],
      "source": [
        "class GraphVisualization:\n",
        "    def __init__(self):\n",
        "\n",
        "        # visual is a list which stores all\n",
        "        # the set of edges that constitutes a\n",
        "        # graph\n",
        "        self.visual = []\n",
        "\n",
        "    # addEdge function inputs the vertices of an\n",
        "    # edge and appends it to the visual list\n",
        "    def addEdge(self, a, b):\n",
        "        temp = [a, b]\n",
        "        self.visual.append(temp)\n",
        "\n",
        "    # In visualize function G is an object of\n",
        "    # class Graph given by networkx G.add_edges_from(visual)\n",
        "    # creates a graph with a given list\n",
        "    # nx.draw_networkx(G) - plots the graph\n",
        "    # plt.show() - displays the graph\n",
        "    def visualize(self):\n",
        "        G = nx.Graph()\n",
        "        G.add_edges_from(self.visual)\n",
        "        nx.draw_networkx(G)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVwH36R8wxcL"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# PREPROCESSING\n",
        "- delete all rows where (speaker == \"[deleted]\" or speaker === \"DeltaBot\" or text == \"[deleted]\")\n",
        "- remove \"stopwords\", replace links with \"URL\", replace emojis\n",
        "- add a column to C called \"embeddings\"\n",
        "- assign the 0 (NO DELTA) label to all utterances\n",
        "- get all utterances containing ∆:\n",
        "  - assign the 1 (DELTA) label to the \"reply_to\" ID. (Do not create an embedding of the ∆ utterance)\n",
        "- remove the utterance containing \"∆\" or \"#8710;\" or \"!delta\" from the C\n",
        "- the first utterance (the root one) has timestamp = None, replace it with timestamp=0\n",
        "- sort C by timestamp => sort_values(by=['timestamp'])\n",
        "'''\n",
        "def preprocessConversationDf(cDf):\n",
        "  # delete all rows where (speaker == \"[deleted]\" or speaker === \"DeltaBot\" or text == \"[deleted]\")\n",
        "  cDf = cDf.loc[(cDf['speaker'] != '[deleted]') & (cDf['speaker'] != 'DeltaBot') & (cDf['text'] != '[deleted]')]\n",
        "\n",
        "  cDf['utterance_label'] = np.zeros(cDf.shape[0]) # 0 is NO DELTA\n",
        "\n",
        "  cdfDelta = cDf.loc[(cDf['text'].str.contains('∆')) | (cDf['text'].str.contains('#8710;')) | (cDf['text'].str.contains('!delta'))]\n",
        "  deltaIndeces = []\n",
        "  for index, row in cdfDelta.iterrows():\n",
        "    replyTo = row['reply_to']\n",
        "    cDf.at[replyTo, 'utterance_label'] = 1 # 1 is DELTA\n",
        "    deltaIndeces.append(index)\n",
        "\n",
        "  cDf.drop(index=deltaIndeces, inplace = True)\n",
        "  cDf['text'] = cDf['text'].map(preprocess_comment)\n",
        "  cDfTimestampNoneIndex = cDf.loc[cDf['timestamp'].isna()]\n",
        "\n",
        "  if cDfTimestampNoneIndex.shape[0] > 0:\n",
        "    cDf.at[cDfTimestampNoneIndex.iloc[0].name, 'timestamp'] = 0\n",
        "  # remove all rows with nan cells\n",
        "  cDf = cDf.dropna(subset=['speaker', 'text'])\n",
        "\n",
        "  return cDf.sort_values(by=['timestamp'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlYHkt8B5ZoB"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# CONVERSATION GRAPH CREATION\n",
        "- for each utterance\n",
        "   - create it's embedding representation and add it to the dataframe column \"embeddings\"\n",
        "   - link it to past utterances (with a max, e.g. max 5) of the same \"speaker\"\n",
        "   - link it to past past utterances (with a max, e.g. max 10) by timestamp, excluding same-speaker utterances\n",
        "   - link the first post\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9QMEo1Pw2zZ"
      },
      "outputs": [],
      "source": [
        "## START CREATE RELATIONSHIPS (LINKS) FUNCTIONS\n",
        "# relationships are the linked edges of a node (represented by index)\n",
        "# index is the index of the node which we want to get the relationships from\n",
        "\n",
        "# gets the relationships based on the number of \"past and future numbers\" by timestamp\n",
        "# (i.e. does not consider subtrees in the conversation)\n",
        "def getRelationshipsByTimestamp(index, cDf, pastSpeakerMax = 3, pastUtterancesMax = 5, futureSameSpeakerMax = 3, futureUtterancesMax = 5):\n",
        "  relationships = set()\n",
        "  utterance = cDf.loc[index]\n",
        "\n",
        "  # comments coming from the same speaker\n",
        "  sameSpeakerCdf = cDf.loc[cDf['speaker'] == utterance.speaker]\n",
        "\n",
        "  # PAST UTTERANCES LINK\n",
        "  # get the previous utterances by timestamp of the same speaker. Past with respect to \"index\" param\n",
        "  # get the location of \"index element\" relative to the new same speaker dataframe\n",
        "  sameSpeakerLoc = sameSpeakerCdf.index.get_loc(index)\n",
        "\n",
        "  sameSpeakerCdfPastUtterances = sameSpeakerCdf.iloc[max(0, sameSpeakerLoc - pastSpeakerMax):sameSpeakerLoc]\n",
        "  relationships.update(sameSpeakerCdfPastUtterances.index)\n",
        "\n",
        "  # get the location of \"index element\" relative to the cDf dataframe\n",
        "  loc = cDf.index.get_loc(index)\n",
        "  pastUtterancesByTimestamp = cDf.iloc[max(0, loc - pastUtterancesMax):loc]\n",
        "  relationships.update(pastUtterancesByTimestamp.index)\n",
        "\n",
        "  # FUTURE UTTERANCES LINK\n",
        "  # get the next utterances by timestamp of the same speaker. Future with respect to \"index\" param\n",
        "  # get the location of \"index element\" relative to the new same speaker dataframe\n",
        "  indexOfFutureLimit = min(sameSpeakerLoc + futureSameSpeakerMax, sameSpeakerCdf.shape[0])\n",
        "  sameSpeakerCdfFutureUtterances = sameSpeakerCdf.iloc[sameSpeakerLoc: indexOfFutureLimit]\n",
        "  relationships.update(sameSpeakerCdfFutureUtterances.index)\n",
        "\n",
        "  # get the past previous utterances by timestamp. Past with respect to \"index\" param\n",
        "  # get the location of \"index element\" relative to the cDf dataframe\n",
        "  loc = cDf.index.get_loc(index)\n",
        "  indexOfFutureLimit = min(loc + futureUtterancesMax, cDf.shape[0])\n",
        "  futureUtterancesByTimestamp = cDf.iloc[loc: indexOfFutureLimit]\n",
        "  relationships.update(futureUtterancesByTimestamp.index)\n",
        "\n",
        "  # add a link also to the first post (the one starting the conversation)\n",
        "  # AND add self connection (check GNNs theory to understand why is useful)\n",
        "  relationships.update({cDf.iloc[0].name, index})\n",
        "\n",
        "  return relationships\n",
        "\n",
        "\n",
        "# returns relationships based on whether a text is a \"reply to\" another one\n",
        "def getRelationshipsByReplyTo(index, cDf, pastSameSpeakerMax = 3, pastUtterancesMax = 5, futureSameSpeakerMax = 3, futureUtterancesMax = 5):\n",
        "  relationships = set()\n",
        "  utterance = cDf.loc[index]\n",
        "  pastSameSpeakerCount = 0\n",
        "  pastUtteranceCount = 0\n",
        "  futureSameSpeakerCount = 0\n",
        "  futureUtteranceCount = 0\n",
        "\n",
        "  dfLength = cDf.shape[0]\n",
        "\n",
        "  # GET PAST UTTERANCES BY REPLY TO\n",
        "  currentUtterance = utterance\n",
        "  lastUtteranceIndex = cDf.iloc[cDf.shape[0] - 1].name\n",
        "  while pastSameSpeakerCount < pastSameSpeakerMax or pastUtteranceCount < pastUtterancesMax:\n",
        "    utteranceReplyToIndex = currentUtterance.reply_to\n",
        "\n",
        "    if utteranceReplyToIndex == None:\n",
        "      break\n",
        "\n",
        "    try:\n",
        "      utteranceReplyTo = cDf.loc[utteranceReplyToIndex]\n",
        "    except:\n",
        "      # happening when a comment has been deleted\n",
        "      break\n",
        "\n",
        "    # if the number of past utterances is achieved but the speaker is not the same we have no interest in adding\n",
        "    # the utterance index to the relationships of the considered utterances\n",
        "    if pastUtteranceCount >= pastUtterancesMax and utteranceReplyTo.speaker != currentUtterance.speaker:\n",
        "      currentUtterance = utteranceReplyTo\n",
        "      continue\n",
        "\n",
        "    relationships.add(utteranceReplyToIndex)\n",
        "\n",
        "    if utteranceReplyTo.speaker == currentUtterance.speaker:\n",
        "      pastSameSpeakerCount += 1\n",
        "\n",
        "    if pastSameSpeakerCount != pastSameSpeakerMax:\n",
        "      pastUtteranceCount += 1\n",
        "\n",
        "    currentUtterance = utteranceReplyTo\n",
        "\n",
        "  # get the location of \"index element\" relative to the cDf dataframe\n",
        "  currentUtteranceLoc = cDf.index.get_loc(index)\n",
        "  # GET FUTURE UTTERANCES BY REPLY TO\n",
        "  currentUtterance = utterance\n",
        "  while futureSameSpeakerCount < futureSameSpeakerMax and futureUtteranceCount < futureUtterancesMax:\n",
        "    if currentUtteranceLoc + 1 >= dfLength:\n",
        "      break\n",
        "\n",
        "    nextUtterance = cDf.iloc[currentUtteranceLoc + 1]\n",
        "\n",
        "    if nextUtterance.reply_to == currentUtterance.name:\n",
        "      relationships.add(nextUtterance.name)\n",
        "\n",
        "      if nextUtterance.speaker == currentUtterance.speaker:\n",
        "        futureSameSpeakerCount += 1\n",
        "      else:\n",
        "        futureUtteranceCount += 1\n",
        "\n",
        "    currentUtterance = nextUtterance\n",
        "    currentUtteranceLoc += 1\n",
        "\n",
        "  return relationships\n",
        "\n",
        "\n",
        "# returns relationships based on the similarity between text\n",
        "def getRelationshipsBySimilarity(index, cDf, similarityThreshold = 0.5, similarityFunction = jaccard_similarity):\n",
        "  relationships = set()\n",
        "  utterance = cDf.loc[index]\n",
        "  utteranceText = utterance.text\n",
        "\n",
        "  # this function is defined inside getRelationshipsBySimilarity cause it needs to access utteranceText\n",
        "  def similarityIsInThreshold(row):\n",
        "    tr = similarityThreshold\n",
        "    return similarityFunction(row.text, utteranceText) > tr and similarityFunction(row.text, utteranceText) != 1\n",
        "\n",
        "  mask = cDf.apply(similarityIsInThreshold, axis=1)\n",
        "  dfAboveSimilarityThreshold = cDf[mask]\n",
        "\n",
        "  relationships.update(dfAboveSimilarityThreshold.index)\n",
        "\n",
        "  return relationships\n",
        "\n",
        "## END CREATE RELATIONSHIPS (LINKS) FUNCTIONS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvmGFaHdF5Wk"
      },
      "outputs": [],
      "source": [
        "# it calulates tensors to be passed to the model\n",
        "# this is useful to calculate node features tensors and edge attributes tensors\n",
        "def calculateTensorList(cDf, attributeFunctions, edgesAsIndicesList = None):\n",
        "  attributes = []\n",
        "  if len(attributeFunctions) > 0:\n",
        "    for attrFunction in attributeFunctions:\n",
        "      # a function might return a tensor or a list\n",
        "      if edgesAsIndicesList == None:\n",
        "        attrMaybeTensor = attrFunction(cDf)\n",
        "      else:\n",
        "        attrMaybeTensor = attrFunction(cDf, edgesAsIndicesList)\n",
        "\n",
        "      # transform the attribute in a tensor only if needed\n",
        "      if not torch.is_tensor(attrMaybeTensor):\n",
        "        attr_el = np.array(attrMaybeTensor)\n",
        "\n",
        "        if type(attrMaybeTensor[0]) is not list:\n",
        "          attr_el_tensor = torch.tensor(attr_el, dtype=torch.float).unsqueeze(1)\n",
        "        else:\n",
        "          attr_el_tensor = torch.tensor(attr_el, dtype=torch.float)\n",
        "      else:\n",
        "        attr_el_tensor = attrMaybeTensor\n",
        "\n",
        "      #print('edge_attr_el_tensor shape of func ' + edgeAttrFunction.__name__)\n",
        "      #print([edge_attr_el_tensor.shape, edge_attr_el_tensor.min(), edge_attr_el_tensor.max()])\n",
        "      attributes.append(attr_el_tensor)\n",
        "\n",
        "  return attributes\n",
        "\n",
        "def getEdgeAttributes(cDf, edgesAsIndicesList):\n",
        "  return calculateTensorList(cDf, edgeAttributeFunctions, edgesAsIndicesList)\n",
        "\n",
        "def getNodeFeatures(cDf):\n",
        "  return calculateTensorList(cDf, nodeFeaturesFunctions)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lWQ6rkX1muKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ymke_4wK5kM-"
      },
      "outputs": [],
      "source": [
        "def plotConversation(edges):\n",
        "  G = GraphVisualization()\n",
        "  for i, edge in edges:\n",
        "    G.addEdge(i, edge)\n",
        "\n",
        "  G.visualize()\n",
        "\n",
        "def getSpeakerNameInConversation(speaker, conversationId):\n",
        "  return speaker + \"-\" + conversationId\n",
        "\n",
        "def createConversationGraph(cDf, conversationId, showPlot):\n",
        "  node_labels = []\n",
        "  utterances_speakers = []\n",
        "  edges = set()\n",
        "  edgesAsIndices = set()\n",
        "\n",
        "  if pss != 0 or pds != 0 or fss != 0 or fds != 0 or 'similarity' in fetchMode:\n",
        "    # Create a dictionary to map utterance IDs to node indices\n",
        "    node_index_map = {}\n",
        "\n",
        "    j = 0\n",
        "    for idx, utt in cDf.iterrows():\n",
        "      node_index_map[idx] = j\n",
        "      j += 1\n",
        "\n",
        "  try:\n",
        "    i = 0\n",
        "    for index, utterance in cDf.iterrows():\n",
        "      #node_features.append(utterance.embeddings)\n",
        "\n",
        "      # 2. Define the edge indices based on the graph structure and the node labels\n",
        "      node_labels.append(utterance.utterance_label)\n",
        "      speakerInConversation = getSpeakerNameInConversation(utterance.speaker, conversationId)\n",
        "      utterances_speakers.append(speakerInConversation)\n",
        "\n",
        "      relationshipsSet = set()\n",
        "\n",
        "      # do not add any links for the baseline model\n",
        "      if pss != 0 or pds != 0 or fss != 0 or fds != 0 or 'similarity' in fetchMode:\n",
        "        # Determine the relationships (edges) for each utterance\n",
        "        if 'timestamp' in fetchMode:\n",
        "          relationshipsByTimestamp = getRelationshipsByTimestamp(index, cDf, pss, pds, fss, fds)\n",
        "          relationshipsSet.update(relationshipsByTimestamp)\n",
        "\n",
        "        if 'reply_to' in fetchMode:\n",
        "          relationshipsByReplyTo = getRelationshipsByReplyTo(index, cDf, pss, pds, fss, fds)\n",
        "          relationshipsSet.update(relationshipsByReplyTo)\n",
        "\n",
        "        if 'similarity' in fetchMode:\n",
        "          relationshipsBySimilarity = getRelationshipsBySimilarity(index, cDf)\n",
        "          relationshipsSet.update(relationshipsBySimilarity)\n",
        "\n",
        "        relationships = list(relationshipsSet)\n",
        "\n",
        "        # Add the edges to the list\n",
        "        for related_utterance_index in relationships:\n",
        "            if related_utterance_index in node_index_map:\n",
        "                related_utterance_i = node_index_map[related_utterance_index]\n",
        "                edges.add((i, related_utterance_i))\n",
        "                edgesAsIndices.add((index, related_utterance_index))\n",
        "\n",
        "      i += 1\n",
        "\n",
        "    edgesList = list(edges)\n",
        "    edgesAsIndicesList = list(edgesAsIndices)\n",
        "\n",
        "    node_features = getNodeFeatures(cDf)\n",
        "    edge_attributes = getEdgeAttributes(cDf, edgesAsIndicesList)\n",
        "\n",
        "    if showPlot:\n",
        "      print('')\n",
        "      plotConversation(edgesList)\n",
        "      print(edgesList)\n",
        "\n",
        "    # 4. Create a Data object for the conversation\n",
        "    x = torch.cat(node_features, dim=1)\n",
        "    edge_index = torch.tensor(edgesList, dtype=torch.long).t().contiguous()\n",
        "    y = torch.tensor(node_labels, dtype=torch.long)\n",
        "\n",
        "    if len(edgeAttributeFunctions) > 0:\n",
        "      # concatenating on the 2nd dimenstion all edge attributes\n",
        "      edge_attr = torch.cat(edge_attributes, dim=1)\n",
        "      data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y, utterances_speakers=utterances_speakers)\n",
        "    else:\n",
        "      data = Data(x=x, edge_index=edge_index, y=y, utterances_speakers=utterances_speakers)\n",
        "    return data\n",
        "  except Exception as e:\n",
        "    print('EXC: ---')\n",
        "    print(e)\n",
        "    print(traceback.format_exc())\n",
        "    print('')\n",
        "    return False\n",
        "\n",
        "def addLabelOneToAllUtterancesOfSpeakersAwarded(cDf):\n",
        "  speakersAwardedInConversation = set()\n",
        "  for index, utterance in cDf.iterrows():\n",
        "    if utterance['utterance_label'] == 1:\n",
        "      speakersAwardedInConversation.add(utterance.speaker)\n",
        "\n",
        "  for index, utt in cDf.iterrows():\n",
        "    # if the speaker is an awarded one and the label is set to 0 and the message is longer than 200 characters\n",
        "    # then label the comment as persuasive; the idea behind the 200 characters check is that only longer comments\n",
        "    # are able to be persuasive\n",
        "    if utt.speaker in speakersAwardedInConversation and utt['utterance_label'] == 0 and len(utt.text) > 200:\n",
        "      cDf.at[index, 'utterance_label'] = 0.5\n",
        "\n",
        "  return cDf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOaC1urevoSs"
      },
      "outputs": [],
      "source": [
        "i = 0\n",
        "maxConversations = 3051\n",
        "#maxConversations = 300\n",
        "#maxConversations = 10\n",
        "\n",
        "persuasiveSpeakerCommentsLabeledAs1 = False\n",
        "\n",
        "#allConversationIds = corpus.get_conversation_ids()\n",
        "#conversationIds = allConversationIds[i:maxConversations]\n",
        "\n",
        "# show the plot only if there is a single conversation processed\n",
        "if maxConversations < 5:\n",
        "  showPlot = True\n",
        "else:\n",
        "  showPlot = False\n",
        "\n",
        "data_list = []\n",
        "convIds = set()\n",
        "numberOfDuplicatesConvs = 0\n",
        "\n",
        "preprocessedDatasetDir = baseDir + 'preprocessed-df/'\n",
        "\n",
        "# Process each conversation\n",
        "for filenameWithExtension in os.listdir(preprocessedDatasetDir):\n",
        "#for conversationId in conversationIds:\n",
        "    if filenameWithExtension == '.ipynb_checkpoints':\n",
        "      continue\n",
        "\n",
        "    # the file name is equal to the conversationId\n",
        "    conversationId = filenameWithExtension.replace('.json', '')\n",
        "\n",
        "    # if the conversation is stored as a duplicate, continue\n",
        "    if '(1)' in conversationId:\n",
        "      continue\n",
        "\n",
        "    if conversationId in convIds:\n",
        "      numberOfDuplicatesConvs += 1\n",
        "      print('Duplicate conversation ' + str(conversationId))\n",
        "      continue\n",
        "\n",
        "    if i == maxConversations:\n",
        "      break\n",
        "\n",
        "    convIds.add(conversationId)\n",
        "\n",
        "    conversationFileName = fullDirectory + '/instance_' + str(i) + '.pt'\n",
        "\n",
        "    try:\n",
        "      if not speakerLevelLabel:\n",
        "        # load the preprocessed result for a specific graph structure\n",
        "        conversationDataGraph = torch.load(conversationFileName)\n",
        "      else:\n",
        "        raise Exception('need to read the file from json as the label must be set at speaker level')\n",
        "    except:\n",
        "      try:\n",
        "        # load the preprocessed df of all comments (this does not include their graph structure)\n",
        "        conversationDfPreprocessed = pd.read_json(preprocessedDatasetDir + conversationId + '.json')\n",
        "      except:\n",
        "        # given a conversation dataframe C having the following columns ['timestamp', 'speaker','reply_to', 'text']\n",
        "        # (for now do not add 'meta.author_flair_text' which is the number of deltas awarded in the past)\n",
        "\n",
        "        conversation = corpus.get_conversation(conversationId)\n",
        "        conversationDf = conversation.get_utterances_dataframe()[['timestamp', 'speaker','reply_to', 'text']]\n",
        "        try:\n",
        "          conversationDfPreprocessed = preprocessConversationDf(conversationDf)\n",
        "          conversationDfPreprocessed.to_json(baseDir + 'preprocessed-df/' + conversationId + '.json')\n",
        "        except Exception as e:\n",
        "          print('EXC conversationDfPreprocessed: ---')\n",
        "          print(e)\n",
        "          print(traceback.format_exc())\n",
        "          print('')\n",
        "\n",
        "      if speakerLevelLabel and persuasiveSpeakerCommentsLabeledAs1:\n",
        "          conversationDfPreprocessed = addLabelOneToAllUtterancesOfSpeakersAwarded(conversationDfPreprocessed)\n",
        "\n",
        "      conversationDataGraph = createConversationGraph(conversationDfPreprocessed, conversationId, showPlot)\n",
        "\n",
        "      # we want to save the file only for labels put at the comment level, to avoid confusion in the files\n",
        "      #if not speakerLevelLabel:\n",
        "          #conversationDfPreprocessed.to_json(baseDir + 'preprocessed-df/' + conversationId + '.json')\n",
        "\n",
        "      if conversationDataGraph and not speakerLevelLabel:\n",
        "        torch.save(conversationDataGraph, conversationFileName)\n",
        "\n",
        "    if conversationDataGraph:\n",
        "      data_list.append(conversationDataGraph)\n",
        "\n",
        "    if i % 300 == 0 and i != 0:\n",
        "      print('')\n",
        "      print(i)\n",
        "\n",
        "    i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osk438HPLPaT"
      },
      "outputs": [],
      "source": [
        "# conversationDfPreprocessed[['speaker','utterance_label']] (print this for debugging speaker level labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrFWBzz4wUsK"
      },
      "outputs": [],
      "source": [
        "len(data_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VN1gyI-bMbPQ"
      },
      "outputs": [],
      "source": [
        "speakersMapCount = {}\n",
        "\n",
        "dataListIndex = 0\n",
        "for data_el in data_list:\n",
        "  for speaker in data_el.utterances_speakers:\n",
        "      if speaker not in speakersMapCount:\n",
        "        speakersMapCount[speaker] = [dataListIndex]\n",
        "      elif dataListIndex not in speakersMapCount[speaker]:\n",
        "        speakersMapCount[speaker].append(dataListIndex)\n",
        "  dataListIndex += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71a8WWjhMxCb"
      },
      "outputs": [],
      "source": [
        "sameConvs = set()\n",
        "speakerDuplicates = 0\n",
        "for speaker in speakersMapCount.items():\n",
        "  if len(speaker[1]) > 1:\n",
        "    sameConvs.add((speaker[1][0], speaker[1][1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaWfzkuKYjy8"
      },
      "outputs": [],
      "source": [
        "sameConvs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7H0egW7A2TV"
      },
      "outputs": [],
      "source": [
        "# Variables to compute a random baseline classifier\n",
        "randomLabel = False\n",
        "\n",
        "ratioOfNonDeltaInTheDataset = 0.9873\n",
        "ratioOfDeltaInTheDataset = 0.0127\n",
        "labelTypes = [0,1]\n",
        "\n",
        "# Split the dataset into train, validation, and test sets\n",
        "train_size = int(0.7 * len(data_list))\n",
        "val_size = int(0.2 * len(data_list))\n",
        "test_size = len(data_list) - train_size - val_size\n",
        "if randomLabel == True:\n",
        "  for data in data_list:\n",
        "    lengthOfLabelsArray = len(data.y)\n",
        "    randomLabels = random.choices(labelTypes, weights = [ratioOfNonDeltaInTheDataset,ratioOfDeltaInTheDataset], k = lengthOfLabelsArray)\n",
        "    randomLabelsRensor = torch.Tensor(randomLabels)\n",
        "    data.y = randomLabelsRensor.long()\n",
        "\n",
        "train_data = data_list[:train_size]\n",
        "val_data = data_list[train_size:train_size+val_size]\n",
        "test_data = data_list[train_size+val_size:]\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4YCRBzwNXVB"
      },
      "outputs": [],
      "source": [
        "#train_data[1].edge_attr.size(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgYJ-ufDhQwP"
      },
      "outputs": [],
      "source": [
        "def modelSupportsEdgeAttributes(model):\n",
        "  modelName = type(model).__name__\n",
        "  if modelName == 'SplineNN' or modelName == 'DeeperGCN':\n",
        "    return True\n",
        "\n",
        "  return False\n",
        "\n",
        "# Step 2: Train, Test, and Validate a GCN with the constructed dataset\n",
        "# Create the model instance\n",
        "\n",
        "dimensionalityOfNodeFeatures = 0\n",
        "nodeFeaturesFunctionNames = [f.__name__ for f in nodeFeaturesFunctions]\n",
        "for f_name in nodeFeaturesFunctionNames:\n",
        "  dimensionalityOfNodeFeatures += dimensionalityMap[f_name]\n",
        "\n",
        "print('nodes features dimensionality: ' + str(dimensionalityOfNodeFeatures))\n",
        "\n",
        "in_channels = dimensionalityOfNodeFeatures  # Define the input feature dimensionality\n",
        "hidden_channels = 80  # Define the dimensionality of hidden layers\n",
        "out_channels = 2  # Two classes: DELTA (1) and NON-DELTA (0) nodes\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('device is: ' + str(device))\n",
        "\n",
        "num_layers = 2\n",
        "\n",
        "#model = GCN(in_channels, hidden_channels, num_layers, out_channels)\n",
        "model = GSAGE(in_channels, hidden_channels, out_channels).to(device)\n",
        "#model = GSAGEv2(in_channels, hidden_channels, out_channels, num_layers)\n",
        "#model = GAT(in_channels, hidden_channels, num_layers, out_channels, dropout = 0.2)\n",
        "#model = Baseline(in_channels, hidden_channels, out_channels).to(device)\n",
        "\n",
        "# MODELS supporting edge attributes\n",
        "\n",
        "dataSample = train_data[0]\n",
        "num_layers_deep = 16\n",
        "#model = DeeperGCN(dataSample, in_channels, hidden_channels, out_channels, num_layers_deep).to(device)\n",
        "\n",
        "#model = SplineNN(in_channels, hidden_channels, out_channels, dim=len(edgeAttributeFunctions)).to(device)\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "print(len(train_data))\n",
        "\n",
        "train_targets = []\n",
        "for data in train_data:\n",
        "  data = data.to(device)\n",
        "  train_targets.extend(data.y.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_drmLkTUwjDc"
      },
      "outputs": [],
      "source": [
        "def updateSpeakerMapToLabel(data, speakersMapToLabel, probabilities, predicted_labels):\n",
        "  i = 0\n",
        "  for uttSpeakers in data.utterances_speakers:\n",
        "    for speaker in uttSpeakers:\n",
        "      if speaker not in speakersMapToLabel:\n",
        "        speakersMapToLabel[speaker] = {\n",
        "            \"probabilities\": [],\n",
        "            \"predictions\": [],\n",
        "            \"ground_truths\": [],\n",
        "            \"speaker_label\": 0,\n",
        "            \"probabilities_avg\": {\n",
        "                \"0\": 0,\n",
        "                \"1\": 0\n",
        "            }\n",
        "        }\n",
        "\n",
        "      ground_truth = data.y[i].tolist()\n",
        "      speakersMapToLabel[speaker]['probabilities'].append(probabilities[i].tolist())\n",
        "      speakersMapToLabel[speaker]['predictions'].append(predicted_labels[i].tolist())\n",
        "      speakersMapToLabel[speaker]['ground_truths'].append(ground_truth)\n",
        "\n",
        "      if ground_truth == 1:\n",
        "        speakersMapToLabel[speaker]['speaker_label'] = 1\n",
        "\n",
        "      i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBLsfdIRvdS5"
      },
      "outputs": [],
      "source": [
        "def isLastEpoch(numEpochs, currentEpoch):\n",
        "  return currentEpoch + 1 == numEpochs\n",
        "\n",
        "def getMRR(y_true, y_pred):\n",
        "  # Sort the predictions and get the indices that would sort them in descending order\n",
        "  sorted_indices = torch.argsort(y_pred, descending=True)\n",
        "\n",
        "  # Initialize an empty list to store reciprocal ranks\n",
        "  reciprocal_ranks = []\n",
        "\n",
        "  # Calculate the reciprocal rank for each correct prediction\n",
        "  for i in range(len(y_true)):\n",
        "      if y_true[sorted_indices[i]] == 1:\n",
        "          reciprocal_rank = 1 / (i + 1)  # Add 1 because indices are 0-based\n",
        "          reciprocal_ranks.append(reciprocal_rank)\n",
        "\n",
        "  # Calculate the Mean Reciprocal Rank\n",
        "  if len(reciprocal_ranks) > 0:\n",
        "      return sum(reciprocal_ranks) / len(reciprocal_ranks)\n",
        "  else:\n",
        "      return 0.0\n",
        "\n",
        "# set the number of epochs\n",
        "#num_epochs = 60\n",
        "#num_epochs = 30\n",
        "num_epochs = 20\n",
        "\n",
        "# Initialize lists to store losses\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "speakersMapToTrain = {}\n",
        "speakersMapToLabel = {}\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = compute_class_weight('balanced', classes=[0, 1], y=train_targets)\n",
        "\n",
        "# Convert class weights to a PyTorch tensor\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "# Training loop\n",
        "model.train()\n",
        "\n",
        "printed = False\n",
        "for epoch in range(num_epochs):\n",
        "  for data in train_loader:\n",
        "      data = data.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      if len(edgeAttributeFunctions) > 0 and modelSupportsEdgeAttributes(model):\n",
        "        out = model(data.x, data.edge_index, data.edge_attr)\n",
        "      else:\n",
        "        out = model(data.x, data.edge_index)\n",
        "\n",
        "      predicted_labels = out.argmax(dim=1)\n",
        "\n",
        "      probabilities = F.softmax(out, dim=1)\n",
        "\n",
        "      # Compute loss\n",
        "      loss = F.cross_entropy(out, data.y, weight=class_weights)\n",
        "      #loss = F.cross_entropy(out, data.y)\n",
        "\n",
        "      # Backpropagation\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "  # Evaluation on validation set\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      val_predictions = []\n",
        "      val_targets = []\n",
        "\n",
        "      val_correct_1s = 0\n",
        "      val_correct_0s = 0\n",
        "\n",
        "      for data in val_loader:\n",
        "          data = data.to(device)\n",
        "          if len(edgeAttributeFunctions) > 0 and modelSupportsEdgeAttributes(model):\n",
        "            out = model(data.x, data.edge_index, data.edge_attr)\n",
        "          else:\n",
        "            out = model(data.x, data.edge_index)\n",
        "\n",
        "          predicted_labels = out.argmax(dim=1)\n",
        "\n",
        "          probabilities = F.softmax(out, dim=1)\n",
        "\n",
        "          yAsList = data.y.tolist()\n",
        "\n",
        "          if(isLastEpoch and not yAsList.count(1) == 0):\n",
        "            updateSpeakerMapToLabel(data, speakersMapToLabel, probabilities, predicted_labels)\n",
        "\n",
        "          val_predictions.extend(predicted_labels.tolist())\n",
        "\n",
        "          val_targets.extend(yAsList)\n",
        "\n",
        "          val_correct_1s += ((predicted_labels == 1) & (data.y == 1)).sum().item()\n",
        "          val_correct_0s += ((predicted_labels == 0) & (data.y == 0)).sum().item()\n",
        "\n",
        "\n",
        "      val_predictions = torch.tensor(val_predictions, dtype=torch.long)\n",
        "      val_targets = torch.tensor(val_targets, dtype=torch.long)\n",
        "\n",
        "      val_loss = F.cross_entropy(out, data.y)\n",
        "      val_accuracy = accuracy_score(val_targets, val_predictions)\n",
        "      val_roc_auc = roc_auc_score(val_targets, val_predictions)\n",
        "      val_avg_precision = average_precision_score(val_targets, val_predictions)\n",
        "      val_mrr = getMRR(val_targets, val_predictions)\n",
        "      val_f1 = f1_score(val_targets, val_predictions, average='macro')\n",
        "      val_precision = precision_score(val_targets, val_predictions, average='macro')\n",
        "      val_recall = recall_score(val_targets, val_predictions, average='macro')\n",
        "\n",
        "      val_ratio_1s = val_correct_1s / (val_targets == 1).sum().item()\n",
        "      val_ratio_0s = val_correct_0s / (val_targets == 0).sum().item()\n",
        "\n",
        "  # Evaluation on test set\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      test_predictions = []\n",
        "      test_targets = []\n",
        "      test_correct_1s = 0\n",
        "      test_correct_0s = 0\n",
        "\n",
        "      for data in test_loader:\n",
        "          data = data.to(device)\n",
        "          if len(edgeAttributeFunctions) > 0 and modelSupportsEdgeAttributes(model):\n",
        "            out = model(data.x, data.edge_index, data.edge_attr)\n",
        "          else:\n",
        "            out = model(data.x, data.edge_index)\n",
        "          predicted_labels = out.argmax(dim=1)\n",
        "\n",
        "          probabilities = F.softmax(out, dim=1)\n",
        "          yAsList = data.y.tolist()\n",
        "          if(isLastEpoch and not yAsList.count(1) == 0):\n",
        "            updateSpeakerMapToLabel(data, speakersMapToLabel, probabilities, predicted_labels)\n",
        "\n",
        "          test_predictions.extend(predicted_labels.tolist())\n",
        "\n",
        "          test_targets.extend(yAsList)\n",
        "\n",
        "          test_correct_1s += ((predicted_labels == 1) & (data.y == 1)).sum().item()\n",
        "          test_correct_0s += ((predicted_labels == 0) & (data.y == 0)).sum().item()\n",
        "\n",
        "      test_predictions = torch.tensor(test_predictions, dtype=torch.long)\n",
        "      test_targets = torch.tensor(test_targets, dtype=torch.long)\n",
        "\n",
        "      test_loss = F.cross_entropy(out, data.y)\n",
        "      test_accuracy = accuracy_score(test_targets, test_predictions)\n",
        "      test_roc_auc = roc_auc_score(test_targets, test_predictions)\n",
        "      test_avg_precision = average_precision_score(test_targets, test_predictions)\n",
        "      test_mrr = getMRR(test_targets, test_predictions)\n",
        "      test_f1 = f1_score(test_targets, test_predictions, average='macro')\n",
        "      test_precision = precision_score(test_targets, test_predictions, average='macro')\n",
        "      test_recall = recall_score(test_targets, test_predictions, average='macro')\n",
        "      test_ratio_1s = test_correct_1s / (test_targets == 1).sum().item()\n",
        "      test_ratio_0s = test_correct_0s / (test_targets == 0).sum().item()\n",
        "\n",
        "  train_losses.append(loss.item())\n",
        "  test_losses.append(test_loss.item())\n",
        "\n",
        "  print(\"Epoch:\", epoch+1)\n",
        "\n",
        "print('')\n",
        "print('train_targets')\n",
        "trainTargetsArr = np.array(train_targets)\n",
        "trainUnique, trainCount = np.unique(trainTargetsArr, return_counts=True)\n",
        "dictOfTrainCount = dict(zip(trainUnique, trainCount))\n",
        "print(dictOfTrainCount)\n",
        "print('')\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(\"Validation:\")\n",
        "print(\"Loss:\", val_loss.item())\n",
        "print(\"Accuracy:\", val_accuracy)\n",
        "print(\"AUC-ROC Score:\", val_roc_auc)\n",
        "print(\"Avg Precision:\", val_avg_precision)\n",
        "print(\"MRR:\", val_mrr)\n",
        "print(\"F1 Score:\", val_f1)\n",
        "print(\"Precision:\", val_precision)\n",
        "print(\"Recall:\", val_recall)\n",
        "print(\"Ratio of Correct 1s:\", val_ratio_1s)\n",
        "print(\"Ratio of Correct 0s:\", val_ratio_0s)\n",
        "print('')\n",
        "print('val_predictions')\n",
        "valPredictionArr = val_predictions.numpy()\n",
        "valUniquePred, valCountsPred = np.unique(valPredictionArr, return_counts=True)\n",
        "dictOfValPred = dict(zip(valUniquePred, valCountsPred))\n",
        "print(dictOfValPred)\n",
        "print('val_targets')\n",
        "valTargetsArr = val_targets.numpy()\n",
        "valUniqueTarget, valCountsTarget = np.unique(valTargetsArr, return_counts=True)\n",
        "dictOfValTargets = dict(zip(valUniqueTarget, valCountsTarget))\n",
        "print(dictOfValTargets)\n",
        "print('')\n",
        "\n",
        "print(\"Test:\")\n",
        "print(\"Loss:\", test_loss.item())\n",
        "print(\"Accuracy:\", test_accuracy)\n",
        "print(\"Avg Precision:\", test_avg_precision)\n",
        "print(\"MRR:\", test_mrr)\n",
        "print(\"AUC-ROC Score:\", test_roc_auc)\n",
        "print(\"F1 Score:\", test_f1)\n",
        "print(\"Precision:\", test_precision)\n",
        "print(\"Recall:\", test_recall)\n",
        "print(\"Ratio of Correct 1s:\", test_ratio_1s)\n",
        "print(\"Ratio of Correct 0s:\", test_ratio_0s)\n",
        "print('')\n",
        "\n",
        "print('test_predictions')\n",
        "testPredictionArr = test_predictions.numpy()\n",
        "testUniquePred, testCountsPred = np.unique(testPredictionArr, return_counts=True)\n",
        "dictOfTestPred = dict(zip(testUniquePred, testCountsPred))\n",
        "print(dictOfTestPred)\n",
        "print('test_targets')\n",
        "testTargetsArr = test_targets.numpy()\n",
        "testUniqueTarget, testCountsTarget = np.unique(testTargetsArr, return_counts=True)\n",
        "dictOfTestTarget = dict(zip(testUniqueTarget, testCountsTarget))\n",
        "print(dictOfTestTarget)\n",
        "print('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzNhoRZBI3q9"
      },
      "outputs": [],
      "source": [
        "count = 0\n",
        "notInCount = 0\n",
        "for speaker in speakersMapToLabel.items():\n",
        "  if speaker[0] not in speakersMapToTrain:\n",
        "    notInCount += 1\n",
        "  else:\n",
        "    count += 1\n",
        "\n",
        "[count, notInCount]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faXXtu7hFkeI"
      },
      "outputs": [],
      "source": [
        "auc = roc_auc_score(valTargetsArr, valPredictionArr)\n",
        "\n",
        "print(\"AUC-ROC val:\", auc)\n",
        "\n",
        "auc = roc_auc_score(testTargetsArr, testPredictionArr)\n",
        "\n",
        "print(\"AUC-ROC test:\", auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQnCQDXPr03j"
      },
      "outputs": [],
      "source": [
        "# percentage of nodes that are 1s in the dataset,\n",
        "# a random classifier would therefore classify correctly 1.1% of the nodes\n",
        "#0.01142263759086189"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-N_1iv23atwU"
      },
      "outputs": [],
      "source": [
        "{\n",
        "    \"model_name\": type(model).__name__, # gets the class name\n",
        "    \"layers_size\": [int(in_channels), int(hidden_channels), int(out_channels)],\n",
        "    \"pss\": int(pss),\n",
        "    \"pds\": int(pds),\n",
        "    \"fss\": int(fss),\n",
        "    \"fds\": int(fds),\n",
        "    \"epochs\": int(num_epochs),\n",
        "    \"fetch_mode\": fetchMode,\n",
        "    \"node_features\": [f.__name__ for f in nodeFeaturesFunctions],\n",
        "    \"edge_features\": [f.__name__ for f in edgeAttributeFunctions],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = []\n",
        "y = []\n",
        "opEmbeddings = {}\n",
        "\n",
        "for speaker, attributes in speakersMapToLabel.items():\n",
        "  prob0 = 0\n",
        "  prob1 = 0\n",
        "  probAvg = speakersMapToLabel[speaker]['probabilities_avg']\n",
        "  numOfUtterances = len(attributes['probabilities'])\n",
        "  '''\n",
        "  for probability in attributes['probabilities']:\n",
        "    prob0 += probability[0]\n",
        "    prob1 += probability[1]\n",
        "\n",
        "  probAvg0 = prob0 / numOfUtterances\n",
        "  probAvg1 = prob1 / numOfUtterances\n",
        "  X.append([probAvg0, probAvg1, numOfUtterances])\n",
        "  '''\n",
        "  XtoBeAppended = []\n",
        "  for i in range(5):\n",
        "    try:\n",
        "      XtoBeAppended.append(attributes['probabilities'][i][0])\n",
        "      XtoBeAppended.append(attributes['probabilities'][i][1])\n",
        "    except:\n",
        "      XtoBeAppended.append(0)\n",
        "      XtoBeAppended.append(0)\n",
        "\n",
        "  '''\n",
        "  rightIndex = speaker.rfind('-')\n",
        "  conversationId = speaker[rightIndex + 1: ]\n",
        "  if conversationId not in opEmbeddings:\n",
        "    conversationDfPreprocessed = pd.read_json(preprocessedDatasetDir + conversationId + '.json')\n",
        "    opRow = conversationDfPreprocessed.loc[conversationDfPreprocessed['timestamp'] == 0]\n",
        "    opEmbeddings[conversationId] = opRow['embeddings']\n",
        "\n",
        "  XtoBeAppended = XtoBeAppended + list(opEmbeddings[conversationId])[0]\n",
        "  '''\n",
        "  XtoBeAppended.append(numOfUtterances)\n",
        "\n",
        "\n",
        "  X.append(XtoBeAppended)\n",
        "  y.append(speakersMapToLabel[speaker]['speaker_label'])"
      ],
      "metadata": {
        "id": "in8H6QH6nk-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "metadata": {
        "id": "wwU9hUz6qnLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X_train),len(X_test),len(y_train),len(y_test))"
      ],
      "metadata": {
        "id": "kPF2RsEH8Gaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_test[0]"
      ],
      "metadata": {
        "id": "9NfbFHF59JlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "RSQHnQ62qSAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_test = rf.predict(X_test)"
      ],
      "metadata": {
        "id": "hCcAlWhyqwdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(y_test, y_pred_test)"
      ],
      "metadata": {
        "id": "uANTqYbmqy-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auc = roc_auc_score(y_test, y_pred_test)\n",
        "print(auc)"
      ],
      "metadata": {
        "id": "SHUmEnM08nZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_pred_test))"
      ],
      "metadata": {
        "id": "Ni9FEWrpq4sY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmTViQ3fwLwg"
      },
      "outputs": [],
      "source": [
        "if not speakerLevelLabel:\n",
        "  with open(baseDir + 'experiments.json', 'r') as experimentsJson:\n",
        "    experiments = json.load(experimentsJson)\n",
        "    experiments.append({\n",
        "        \"model_name\": type(model).__name__, # gets the class name\n",
        "        \"layers_size\": [int(in_channels), int(hidden_channels), int(out_channels)],\n",
        "        \"pss\": int(pss),\n",
        "        \"pds\": int(pds),\n",
        "        \"fss\": int(fss),\n",
        "        \"fds\": int(fds),\n",
        "        \"epochs\": int(num_epochs),\n",
        "        \"number_of_layers\": num_layers,\n",
        "        \"fetch_mode\": fetchMode,\n",
        "        \"node_features\": [f.__name__ for f in nodeFeaturesFunctions],\n",
        "        \"edge_features\": [f.__name__ for f in edgeAttributeFunctions],\n",
        "        \"with-stopwords\": True,\n",
        "        \"self_edge\": True,\n",
        "        \"val_accuracy\": float(val_accuracy),\n",
        "        \"val_loss\":  float(val_loss.item()),\n",
        "        \"val_f1score\": float(val_f1),\n",
        "        \"val_precision\": float(val_precision),\n",
        "        \"val_recall\": float(val_recall),\n",
        "        \"val_ratio_of_correct_ones\": float(val_ratio_1s),\n",
        "        \"val_ratio_of_correct_zeros\": float(val_ratio_0s),\n",
        "        \"test_accuracy\": float(test_accuracy),\n",
        "        \"test_loss\": float(test_loss.item()),\n",
        "        \"test_f1score\": float(test_f1),\n",
        "        \"test_precision\": float(test_precision),\n",
        "        \"test_recall\": float(test_recall),\n",
        "        \"test_ratio_of_correct_ones\": float(test_ratio_1s),\n",
        "        \"test_ratio_of_correct_zeros\": float(test_ratio_0s),\n",
        "        \"train_size\": str(list(trainCount)),\n",
        "        \"val_size\": str(list(valCountsTarget)),\n",
        "        \"test_size\": str(list(testCountsTarget))\n",
        "    })\n",
        "\n",
        "  with open(baseDir + 'experiments.json', 'w') as experimentsJson:\n",
        "      json.dump(experiments, experimentsJson, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmSPhO2d6Hoi"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "saveSpeakerResults = True\n",
        "\n",
        "# 2.6% is the number of \"persuasive speakers\" a random model would predict\n",
        "if speakerLevelLabel or saveSpeakerResults:\n",
        "  numOfCorrect = numOf1sCorrect + numOf0sCorrect\n",
        "  numOfMisclassified = numOf0sMisclassified + numOf1sMisclassified\n",
        "  valAndTestAccuracy = numOfCorrect / (numOfCorrect + numOfMisclassified)\n",
        "  # trueP / trueP + FalseP\n",
        "  valAndTestPrecision = numOf1sCorrect / (numOf1sCorrect + numOf0sMisclassified)\n",
        "  # TruePositives / (TruePositives + FalseNegatives)\n",
        "  valAndTestRecall = numOf1sCorrect / (numOf1sCorrect + numOf1sMisclassified)\n",
        "  # F-Measure = (2 * Precision * Recall) / (Precision + Recall)\n",
        "  valAndTestF1score = (2 * valAndTestPrecision * valAndTestRecall) / (valAndTestPrecision + valAndTestRecall)\n",
        "\n",
        "  with open(baseDir + 'experiments-speaker-level.json', 'r') as experimentsSpeakerLevelJson:\n",
        "    experimentsSpeakerLevel = json.load(experimentsSpeakerLevelJson)\n",
        "    toBeAppended = {\n",
        "        \"model_name\": type(model).__name__, # gets the class name\n",
        "        \"layers_size\": [int(in_channels), int(hidden_channels), int(out_channels)],\n",
        "        \"pss\": int(pss),\n",
        "        \"pds\": int(pds),\n",
        "        \"fss\": int(fss),\n",
        "        \"fds\": int(fds),\n",
        "        \"epochs\": int(num_epochs),\n",
        "        \"number_of_layers\": num_layers,\n",
        "        \"fetch_mode\": fetchMode,\n",
        "        \"node_features\": [f.__name__ for f in nodeFeaturesFunctions],\n",
        "        \"edge_features\": [f.__name__ for f in edgeAttributeFunctions],\n",
        "        \"persuasive_speaker_comments_labeled_1\": persuasiveSpeakerCommentsLabeledAs1,\n",
        "        \"num_of_1s_correct\": numOf1sCorrect,\n",
        "        \"num_of_1s_misclassified\": numOf1sMisclassified,\n",
        "        \"ratio_of_correct_ones\": numOf1sCorrect / (numOf1sCorrect + numOf1sMisclassified),\n",
        "        \"num_of_0s_correct\": numOf0sCorrect,\n",
        "        \"num_of_0s_misclassified\": numOf0sMisclassified,\n",
        "        \"ratio_of_correct_zeros\": numOf0sCorrect / (numOf0sCorrect + numOf0sMisclassified),\n",
        "        \"num_of_correctly_classified\": numOfCorrect,\n",
        "        \"number_of_misclassified\": numOfMisclassified,\n",
        "        \"val_and_test_accuracy\": float(valAndTestAccuracy),\n",
        "        \"val_and_test_f1score\": float(valAndTestF1score),\n",
        "        \"val_and_test_precision\": float(valAndTestPrecision),\n",
        "        \"val_and_test_recall\": float(valAndTestRecall)\n",
        "    }\n",
        "    experimentsSpeakerLevel.append(toBeAppended)\n",
        "\n",
        "  print(toBeAppended)\n",
        "  #with open(baseDir + 'experiments-speaker-level.json', 'w') as experimentsSpeakerLevelJson:\n",
        "      #json.dump(experimentsSpeakerLevel, experimentsSpeakerLevelJson, indent=4)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6Z0mhQLnV3t"
      },
      "outputs": [],
      "source": [
        "# Plot the losses\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(test_losses, label='Test Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLco5PdJHgUI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}